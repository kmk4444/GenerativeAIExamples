{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOS4rokxGs+i+yfcCXNd1P6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/LLM/blob/main/Simple_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot"
      ],
      "metadata": {
        "id": "GrvmdhZwe8VO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Requirements.txt**"
      ],
      "metadata": {
        "id": "ctDBzk96hG3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!touch requirements.txt\n",
        "!echo python-dotenv >> requirements.txt\n",
        "!echo openai >> requirements.txt\n",
        "!echo streamlit >> requirements.txt\n",
        "!echo google-generativeai >> requirements.txt\n",
        "!echo anthropic >> requirements.txt\n",
        "!echo cohere >> requirements.txt\n",
        "!echo replicate >> requirements.txt\n"
      ],
      "metadata": {
        "id": "OSjfLQu2fDqx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **terminal / bash komutu**"
      ],
      "metadata": {
        "id": "rxk632Nfh0OT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "gFhxgec9heEP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4039db7c-d1a1-400b-af9d-b8d15577d05e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: watchdog, types-requests, smmap, python-dotenv, h11, fastavro, pydeck, httpcore, gitdb, httpx, gitpython, replicate, openai, cohere, anthropic, streamlit\n",
            "Successfully installed anthropic-0.25.1 cohere-5.2.5 fastavro-1.9.4 gitdb-4.0.11 gitpython-3.1.43 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.17.1 pydeck-0.8.1b0 python-dotenv-1.0.1 replicate-0.25.1 smmap-5.0.1 streamlit-1.33.0 types-requests-2.31.0.20240406 watchdog-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First Chat Application**\n",
        "- We've used OpenAI key."
      ],
      "metadata": {
        "id": "f8pxXXfQMEOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from openai import OpenAI\n",
        "#from google.colab import userdata\n",
        "#userdata.get('OPENAI_APIKEY')\n",
        "\n",
        "client = OpenAI(api_key=\"-------\")\n",
        "\n",
        "# Firstly, chat history was created. We used Session state because we don't want to\n",
        "#lose our historical data every time we refresh the page.\n",
        "\n",
        "if \"messages\" not in st.session_state: # if messages is not created, we'll create Session State message list.\n",
        "  st.session_state.messages = []\n",
        "  st.session_state.messages.append({\"role\":\"system\", \"content\":\"Sen yardımsever bir asistansın\"}) # We append system default prompt. (This is default first message)\n",
        "\n",
        "# It creates answer using user's prompt.\n",
        "def generate_response(prompt):\n",
        "  st.session_state.messages.append({\"role\":\"user\",\"content\":prompt})  # We append user's prompt.\n",
        "\n",
        "  # We reached client object's create method. It provides to use OpenAI SDK. (Create method.)\n",
        "  AI_Response =  client.chat.completions.create(\n",
        "        model=\"gpt-4-1106-preview\",\n",
        "        messages=st.session_state.messages # We sent our message list to OpenAI messages.\n",
        "  )\n",
        "\n",
        "  return AI_Response.choices[0].message.content # We pulled logical data.\n",
        "\n",
        "\n",
        "st.header(\"İlk sohbet botum\") # it is header of the webpage\n",
        "st.divider() # it created a line\n",
        "\n",
        "# Creating a loop to show each message on webpage\n",
        "\n",
        "for message in st.session_state.messages[1:]: # we started [1:] because first data was \"{\"role\":\"system\", \"content\":\"Sen yardımsever bir asistansın\"})\". It is not important\n",
        "  with st.chat_message(message[\"role\"]): # we show system or user.\n",
        "    st.markdown(message[\"content\"]) # we show system content or user prompt.\n",
        "\n",
        "#  st.chat_message provides to show icon.\n",
        "if prompt:= st.chat_input(\"Mesajınızı Giriniz\"): # if user prompts, the code assigns the user's input to the prompt variable.\n",
        "  st.chat_message(\"user\").markdown(prompt) # we show our prompt\n",
        "\n",
        "  # we show openai's answer.\n",
        "  response = generate_response(prompt)\n",
        "  with st.chat_message(\"assistant\"):\n",
        "    st.markdown(response)\n",
        "\n",
        "  st.session_state.append({\"role\":\"assistant\",\"content\":response}) # we append openai answer to session_state list.\n",
        "\n"
      ],
      "metadata": {
        "id": "cKpGNnHymBiq",
        "outputId": "e27cb02a-40e3-46c5-d126-60be258c61dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU5GSjQumWNV",
        "outputId": "c78f810f-4d4f-40d4-db92-957bf035f94c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25h+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.77s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "zXoualvgmps0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbXcQMsTjE4K",
        "outputId": "ec8a6c1b-8d78-41ad-f42c-5abb226ac6ad"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.488s\n",
            "your url is: https://mean-pigs-invent.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}