{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8u96lHOST5UNowqQUG0dK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/LLM/blob/main/Simple_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot"
      ],
      "metadata": {
        "id": "GrvmdhZwe8VO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Requirements.txt**"
      ],
      "metadata": {
        "id": "ctDBzk96hG3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!touch requirements.txt\n",
        "!echo python-dotenv >> requirements.txt\n",
        "!echo openai >> requirements.txt\n",
        "!echo streamlit >> requirements.txt\n",
        "!echo google-generativeai >> requirements.txt\n",
        "!echo anthropic >> requirements.txt\n",
        "!echo cohere >> requirements.txt\n",
        "!echo replicate >> requirements.txt\n"
      ],
      "metadata": {
        "id": "OSjfLQu2fDqx"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **terminal / bash komutu**"
      ],
      "metadata": {
        "id": "rxk632Nfh0OT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "gFhxgec9heEP"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "my_key = userdata.get('OPENAI_APIKEY')\n",
        "client = OpenAI(api_key=my_key)\n",
        "\n",
        "st.chat_input(\"Mesajınızı Giriniz\")\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "    st.session_state.messages.append({\"role\": \"system\", \"content\":\"Sen yardımsever bir asistansın.\"})\n",
        "\n",
        "def generate_response(prompt):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    AI_Response = client.chat.completions.create(\n",
        "\n",
        "        model = \"gpt-4-1106-preview\",\n",
        "        messages=st.session_state.messages\n",
        "    )\n",
        "\n",
        "    return AI_Response.choices[0].message.content\n",
        "\n",
        "\n",
        "st.header(\"İlk Sohbet Botum\")\n",
        "st.divider()\n",
        "\n",
        "for message in st.session_state.messages[1:]:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "if prompt := st.chat_input(\"Mesajınızı Giriniz\"):\n",
        "\n",
        "    st.chat_message(\"user\").markdown(prompt)\n",
        "\n",
        "    response = generate_response(prompt)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(response)\n",
        "\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKpGNnHymBiq",
        "outputId": "2c4db63e-4a86-4fc9-8342-6910db6718e7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU5GSjQumWNV",
        "outputId": "b7884384-da78-44ef-ce18-e4c0982a0f5a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.739s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "zXoualvgmps0"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbXcQMsTjE4K",
        "outputId": "d312d6f1-bcba-473e-e1d9-0134278034a6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.416s\n",
            "your url is: https://chubby-tables-argue.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}