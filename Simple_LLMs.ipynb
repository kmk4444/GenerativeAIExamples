{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1CQuiyB7ndIHx4IY97vzP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/LLM/blob/main/Simple_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot"
      ],
      "metadata": {
        "id": "GrvmdhZwe8VO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Requirements.txt**"
      ],
      "metadata": {
        "id": "ctDBzk96hG3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!touch requirements.txt\n",
        "!echo python-dotenv >> requirements.txt\n",
        "!echo openai >> requirements.txt\n",
        "!echo streamlit >> requirements.txt\n",
        "!echo google-generativeai >> requirements.txt\n",
        "!echo anthropic >> requirements.txt\n",
        "!echo cohere >> requirements.txt\n",
        "!echo replicate >> requirements.txt\n"
      ],
      "metadata": {
        "id": "OSjfLQu2fDqx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **terminal / bash komutu**"
      ],
      "metadata": {
        "id": "rxk632Nfh0OT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "gFhxgec9heEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First Chat Application**\n",
        "- We've used OpenAI key."
      ],
      "metadata": {
        "id": "f8pxXXfQMEOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from openai import OpenAI\n",
        "#from google.colab import userdata\n",
        "#userdata.get('OPENAI_APIKEY')\n",
        "\n",
        "client = OpenAI(api_key=\"----\")\n",
        "\n",
        "# Firstly, chat history was created. We used Session state because we don't want to\n",
        "#lose our historical data every time we refresh the page.\n",
        "\n",
        "if \"messages\" not in st.session_state: # if messages is not created, we'll create Session State message list.\n",
        "  st.session_state.messages = []\n",
        "  st.session_state.messages.append({\"role\":\"system\", \"content\":\"Sen yardımsever bir asistansın\"}) # We append system default prompt. (This is default first message)\n",
        "\n",
        "# It creates answer using user's prompt.\n",
        "def generate_response(prompt):\n",
        "  st.session_state.messages.append({\"role\":\"user\",\"content\":prompt})  # We append user's prompt.\n",
        "\n",
        "  # We reached client object's create method. It provides to use OpenAI SDK. (Create method.)\n",
        "  AI_Response =  client.chat.completions.create(\n",
        "        model=\"gpt-4-1106-preview\",\n",
        "        messages=st.session_state.messages # We sent our message list to OpenAI messages.\n",
        "  )\n",
        "\n",
        "  return AI_Response.choices[0].message.content # We pulled logical data.\n",
        "\n",
        "\n",
        "st.header(\"İlk sohbet botum\") # it is header of the webpage\n",
        "st.divider() # it created a line\n",
        "\n",
        "# Creating a loop to show each message on webpage\n",
        "\n",
        "for message in st.session_state.messages[1:]: # we started [1:] because first data was \"{\"role\":\"system\", \"content\":\"Sen yardımsever bir asistansın\"})\". It is not important\n",
        "  with st.chat_message(message[\"role\"]): # we show system or user.\n",
        "    st.markdown(message[\"content\"]) # we show system content or user prompt.\n",
        "\n",
        "#  st.chat_message provides to show icon.\n",
        "if prompt:= st.chat_input(\"Mesajınızı Giriniz\"): # if user prompts, the code assigns the user's input to the prompt variable.\n",
        "  st.chat_message(\"user\").markdown(prompt) # we show our prompt\n",
        "\n",
        "  # we show openai's answer.\n",
        "  response = generate_response(prompt)\n",
        "  with st.chat_message(\"assistant\"):\n",
        "    st.markdown(response)\n",
        "\n",
        "  st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n"
      ],
      "metadata": {
        "id": "cKpGNnHymBiq",
        "outputId": "89afd89c-6567-4a36-8585-0a95937887d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU5GSjQumWNV",
        "outputId": "c78f810f-4d4f-40d4-db92-957bf035f94c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25h+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.77s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "zXoualvgmps0"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbXcQMsTjE4K",
        "outputId": "13d7d82d-0c5a-4b2a-85e1-35c669a5939f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.837s\n",
            "your url is: https://large-eyes-listen.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Second Chat Application**\n",
        "- We have used claude 2.1 by Anthrop\\c"
      ],
      "metadata": {
        "id": "wdungdBrhfPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile claude_v1.py\n",
        "import anthropic\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "client = anthropic.Anthropic(\n",
        "  api_key = \"-----\",\n",
        ")\n",
        "\n",
        "AI_Response = client.beta.messages.create(\n",
        "    model = \"claude-2.1\",\n",
        "    temperature=0,\n",
        "    max_tokens=256,\n",
        "    messages=[\n",
        "        {\"role\":\"user\", \"content\":\"Mevsimler neden oluşur?\"}\n",
        "    ]\n",
        ")\n",
        "print(AI_Response.content[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "hqDP-IXJhnRZ",
        "outputId": "c16949f9-2531-4105-a0ea-2139fbe1382b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Beta' object has no attribute 'messages'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-1da8ffc71fd0>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m AI_Response = client.beta.messages.create(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"claude-2.1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Beta' object has no attribute 'messages'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile claude_v2.py\n",
        "\n",
        "import anthropic\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "client = anthropic.Anthropic(\n",
        "  api_key = \"-----\",\n",
        ")\n",
        "\n",
        "def generate_response(prompt):\n",
        "\n",
        "    AI_Response = client.beta.messages.create(\n",
        "        model = \"claude-2.1\",\n",
        "        temperature=0,\n",
        "        max_tokens=256,\n",
        "        messages=[\n",
        "            {\"role\":\"user\", \"content\":prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return AI_Response.content[0].text\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "st.header(\"Claude ile İletişim Kurun\")\n",
        "st.divider()\n",
        "\n",
        "prompt = st.text_input(\"Mesajınızı Giriniz:\")\n",
        "submit_btn = st.button(\"Gönder\")\n",
        "\n",
        "if submit_btn:\n",
        "    response = generate_response(prompt)\n",
        "    st.markdown(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gajwJtZUkmve",
        "outputId": "9f30d8a8-2563-4e69-bc10-cac672d783bd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting claude_v2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjbDDbhHhvb0",
        "outputId": "5adc08ff-cd19-4c8f-981f-57ace18d33c4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 2.307s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/claude_v2.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "LdGZ8jowhxVV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq-2TXA_hyTg",
        "outputId": "f48c4415-e616-4ca6-9217-257bf4397e40"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.458s\n",
            "your url is: https://tender-groups-relax.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Third Chat Application**\n",
        "- We used Command by Cohere"
      ],
      "metadata": {
        "id": "FVbDQdWYmbHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile command.py\n",
        "import cohere\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "client = cohere.Client(\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "CUhNS_r2mh7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "id": "ehAi9Tm0noTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/command.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "b3z-xqrNnpqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "piDBWWmknqX0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}