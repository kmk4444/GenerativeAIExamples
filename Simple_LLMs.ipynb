{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrTgtT5j0LZfWKZUQJ5jyj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/LLM/blob/main/Simple_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbot"
      ],
      "metadata": {
        "id": "GrvmdhZwe8VO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This study contains commercial language models.\n",
        "- There are four different models which are Chatgpt, Claude, Command and Gemini.\n",
        "- You have to own API key to utilize these models.\n",
        "- Models are created two versions which are simple and advanced version.\n",
        "- Simple version consists of prompt and output.\n",
        "- Advanced version consists of streamlit, prompt and output.\n",
        "- For Chatgpt model (OpenAI), we used session state to show history.\n",
        "- To run streamlit on Google Colab, we used localtunnel. If you don't understand, you should read localtunnel document."
      ],
      "metadata": {
        "id": "YkipKuWfdb66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Requirements.txt**"
      ],
      "metadata": {
        "id": "ctDBzk96hG3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!touch requirements.txt\n",
        "!echo python-dotenv >> requirements.txt\n",
        "!echo openai >> requirements.txt\n",
        "!echo streamlit >> requirements.txt\n",
        "!echo google-generativeai >> requirements.txt\n",
        "!echo anthropic >> requirements.txt\n",
        "!echo cohere >> requirements.txt\n",
        "!echo replicate >> requirements.txt\n"
      ],
      "metadata": {
        "id": "OSjfLQu2fDqx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **terminal / bash komutu**"
      ],
      "metadata": {
        "id": "rxk632Nfh0OT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "gFhxgec9heEP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First Chat Application**\n",
        "- We've used OpenAI key."
      ],
      "metadata": {
        "id": "f8pxXXfQMEOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from openai import OpenAI\n",
        "#from google.colab import userdata\n",
        "#userdata.get('OPENAI_APIKEY')\n",
        "\n",
        "client = OpenAI(api_key=\"----\")\n",
        "\n",
        "# Firstly, chat history was created. We used Session state because we don't want to\n",
        "#lose our historical data every time we refresh the page.\n",
        "\n",
        "if \"messages\" not in st.session_state: # if messages is not created, we'll create Session State message list.\n",
        "  st.session_state.messages = []\n",
        "  st.session_state.messages.append({\"role\":\"system\", \"content\":\"Sen yardımsever bir asistansın\"}) # We append system default prompt. (This is default first message)\n",
        "\n",
        "# It creates answer using user's prompt.\n",
        "def generate_response(prompt):\n",
        "  st.session_state.messages.append({\"role\":\"user\",\"content\":prompt})  # We append user's prompt.\n",
        "\n",
        "  # We reached client object's create method. It provides to use OpenAI SDK. (Create method.)\n",
        "  AI_Response =  client.chat.completions.create(\n",
        "        model=\"gpt-4-1106-preview\",\n",
        "        messages=st.session_state.messages # We sent our message list to OpenAI messages.\n",
        "  )\n",
        "\n",
        "  return AI_Response.choices[0].message.content # We pulled logical data.\n",
        "\n",
        "\n",
        "st.header(\"İlk sohbet botum\") # it is header of the webpage\n",
        "st.divider() # it created a line\n",
        "\n",
        "# Creating a loop to show each message on webpage\n",
        "\n",
        "for message in st.session_state.messages[1:]: # we started [1:] because first data was \"{\"role\":\"system\", \"content\":\"Sen yardımsever bir asistansın\"})\". It is not important\n",
        "  with st.chat_message(message[\"role\"]): # we show system or user.\n",
        "    st.markdown(message[\"content\"]) # we show system content or user prompt.\n",
        "\n",
        "#  st.chat_message provides to show icon.\n",
        "if prompt:= st.chat_input(\"Mesajınızı Giriniz\"): # if user prompts, the code assigns the user's input to the prompt variable.\n",
        "  st.chat_message(\"user\").markdown(prompt) # we show our prompt\n",
        "\n",
        "  # we show openai's answer.\n",
        "  response = generate_response(prompt)\n",
        "  with st.chat_message(\"assistant\"):\n",
        "    st.markdown(response)\n",
        "\n",
        "  st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n"
      ],
      "metadata": {
        "id": "cKpGNnHymBiq",
        "outputId": "89afd89c-6567-4a36-8585-0a95937887d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU5GSjQumWNV",
        "outputId": "c78f810f-4d4f-40d4-db92-957bf035f94c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25h+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.77s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "zXoualvgmps0"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbXcQMsTjE4K",
        "outputId": "13d7d82d-0c5a-4b2a-85e1-35c669a5939f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.837s\n",
            "your url is: https://large-eyes-listen.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Second Chat Application**\n",
        "- We have used claude 2.1 by Anthrop\\c"
      ],
      "metadata": {
        "id": "wdungdBrhfPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile claude_v1.py\n",
        "import anthropic\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "client = anthropic.Anthropic(\n",
        "  api_key = \"-----\",\n",
        ")\n",
        "\n",
        "AI_Response = client.beta.messages.create(\n",
        "    model = \"claude-2.1\",\n",
        "    temperature=0,\n",
        "    max_tokens=256,\n",
        "    messages=[\n",
        "        {\"role\":\"user\", \"content\":\"Mevsimler neden oluşur?\"}\n",
        "    ]\n",
        ")\n",
        "print(AI_Response.content[0].text)"
      ],
      "metadata": {
        "id": "hqDP-IXJhnRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile claude_v2.py\n",
        "\n",
        "import anthropic\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "client = anthropic.Anthropic(\n",
        "  api_key = \"-----\",\n",
        ")\n",
        "\n",
        "def generate_response(prompt):\n",
        "\n",
        "    AI_Response = client.beta.messages.create(\n",
        "        model = \"claude-2.1\",\n",
        "        temperature=0,\n",
        "        max_tokens=256,\n",
        "        messages=[\n",
        "            {\"role\":\"user\", \"content\":prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return AI_Response.content[0].text\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "st.header(\"Claude ile İletişim Kurun\")\n",
        "st.divider()\n",
        "\n",
        "prompt = st.text_input(\"Mesajınızı Giriniz:\")\n",
        "submit_btn = st.button(\"Gönder\")\n",
        "\n",
        "if submit_btn:\n",
        "    response = generate_response(prompt)\n",
        "    st.markdown(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gajwJtZUkmve",
        "outputId": "9f30d8a8-2563-4e69-bc10-cac672d783bd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting claude_v2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjbDDbhHhvb0",
        "outputId": "5adc08ff-cd19-4c8f-981f-57ace18d33c4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 2.307s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/claude_v2.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "LdGZ8jowhxVV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq-2TXA_hyTg",
        "outputId": "f48c4415-e616-4ca6-9217-257bf4397e40"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.458s\n",
            "your url is: https://tender-groups-relax.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Third Chat Application**\n",
        "- We used Command by Cohere"
      ],
      "metadata": {
        "id": "FVbDQdWYmbHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile command_v1.py\n",
        "import cohere\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "client = cohere.Client(\n",
        "    api_key =\"----\"\n",
        ")\n",
        "\n",
        "AI_Response = client.chat(\n",
        "    model = \"command\",\n",
        "    temperature =0,\n",
        "    max_tokens=256,\n",
        "    chat_history=[\n",
        "        {\"role\":\"USER\",\"message\":\"Yer çekimini kim bulmuştur?\"},\n",
        "        {\"role\":\"CHATBOT\", \"message\":\"Çekim yasalarını formülize eden Sir Isaac Netwon.\"}\n",
        "    ],\n",
        "    message = \"Kendisi Hangi yılda doğöuştur?\"\n",
        ")\n",
        "\n",
        "print(AI_Response.text)"
      ],
      "metadata": {
        "id": "CUhNS_r2mh7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile command_v2.py\n",
        "import cohere\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "client = cohere.Client(\n",
        "    api_key =\"----\"\n",
        ")\n",
        "\n",
        "def generate_response(prompt):\n",
        "  AI_Response = client.chat(\n",
        "      model = \"command\",\n",
        "      temperature =0,\n",
        "      max_tokens=256,\n",
        "      chat_history=[\n",
        "          {\"role\":\"USER\",\"message\":\"Yer çekimini kim bulmuştur?\"},\n",
        "          {\"role\":\"CHATBOT\", \"message\":\"Çekim yasalarını formülize eden Sir Isaac Netwon.\"}\n",
        "      ],\n",
        "      message = prompt\n",
        "  )\n",
        "\n",
        "  return AI_Response.text\n",
        "\n",
        "import streamlit as st\n",
        "st.header(\"Command ile iletişim Kurun\")\n",
        "st.divider()\n",
        "\n",
        "prompt = st.text_input(\"Mesajınızı Giriniz:\") # Sequence question, we follow Isac Newton (kaç yılında doğmuştur?)\n",
        "submit_btn = st.button(\"Gönder\")\n",
        "\n",
        "if submit_btn:\n",
        "  response = generate_response(prompt)\n",
        "  st.markdown(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW5Vb0_Kr5C9",
        "outputId": "9ffa3403-f5e3-4739-b8d8-120c53582ae6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing command_v2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehAi9Tm0noTv",
        "outputId": "80596a2c-3809-46ad-aaac-4fe2d597ea59"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25h+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.583s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/command_v2.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "b3z-xqrNnpqr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piDBWWmknqX0",
        "outputId": "682687b9-fc19-4b42-a410-aae8781ca4ca"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.902s\n",
            "your url is: https://big-rivers-dress.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We developed fourth Chatbot**\n",
        "- We used Gemini Pro by Google AI"
      ],
      "metadata": {
        "id": "d7FPOUR8vmRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile gemini_v1.py\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "genai.configure(\n",
        "  api_key=\"---\"\n",
        ")\n",
        "\n",
        "client = genai.GenerativeModel(\n",
        "    model_name=\"gemini-pro\"\n",
        ")\n",
        "\n",
        "AI_Response = client.generate_content(\n",
        "    \"Mevsimler neden oluşur?\",\n",
        "    generation_config = genai.GenerationConfig( # we arrange parameters using generation_config and genai.configurewhich we described abovend\n",
        "        temperature = 0,\n",
        "        #max_output_tokens=256\n",
        "    )\n",
        ")\n",
        "\n",
        "print(AI_Response.text)"
      ],
      "metadata": {
        "id": "GmpdjgY8vszX",
        "outputId": "1726ae71-62d1-490a-be28-e224b42b16af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mevsimler, Dünya'nın Güneş etrafındaki yörüngesinin ve ekseninin eğikliğinin bir sonucu olarak oluşur.\n",
            "\n",
            "**Dünya'nın Güneş Etrafındaki Yörüngesi:**\n",
            "\n",
            "* Dünya, Güneş'in etrafında eliptik bir yörüngede döner.\n",
            "* Bu yörünge, Dünya'nın Güneş'e olan mesafesinin yıl boyunca değişmesine neden olur.\n",
            "\n",
            "**Dünya'nın Ekseninin Eğikliği:**\n",
            "\n",
            "* Dünya'nın ekseni, yörünge düzlemine göre 23,5 derece eğiktir.\n",
            "* Bu eğiklik, Dünya'nın farklı bölgelerinin Güneş'e göre farklı açılarda konumlanmasına neden olur.\n",
            "\n",
            "**Mevsimlerin Oluşumu:**\n",
            "\n",
            "* **İlkbahar ve Sonbahar Ekinoksları:**\n",
            "    * Dünya'nın ekseni Güneş'e ne doğru ne de uzağa doğru eğik değildir.\n",
            "    * Bu, Dünya'nın her yerinde gün ve gecenin eşit uzunlukta olduğu ekinokslara neden olur.\n",
            "* **Yaz Gündönümü:**\n",
            "    * Kuzey Yarım Küre'de, Dünya'nın ekseni Güneş'e doğru eğiktir.\n",
            "    * Bu, Kuzey Yarım Küre'de günlerin en uzun ve gecelerin en kısa olduğu yaz gündönümüne neden olur.\n",
            "* **Kış Gündönümü:**\n",
            "    * Kuzey Yarım Küre'de, Dünya'nın ekseni Güneş'ten uzağa eğiktir.\n",
            "    * Bu, Kuzey Yarım Küre'de günlerin en kısa ve gecelerin en uzun olduğu kış gündönümüne neden olur.\n",
            "\n",
            "**Mevsimlerin Etkileri:**\n",
            "\n",
            "Mevsimler, Dünya'daki yaşam üzerinde önemli etkilere sahiptir:\n",
            "\n",
            "* **İklim:** Mevsimler, sıcaklık, yağış ve rüzgar modellerinde değişikliklere neden olur.\n",
            "* **Bitki Örtüsü:** Bitkiler, mevsimlere göre büyüme ve uykuda kalma döngülerine sahiptir.\n",
            "* **Hayvan Yaşamı:** Hayvanlar, mevsimlere göre göç eder, kış uykusuna yatar veya üreme davranışlarını değiştirir.\n",
            "* **İnsan Faaliyetleri:** Mevsimler, tarım, turizm ve rekreasyon gibi insan faaliyetlerini etkiler.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gemini_v2.py\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "genai.configure(\n",
        "  api_key=\"---\"\n",
        ")\n",
        "\n",
        "client = genai.GenerativeModel(\n",
        "    model_name=\"gemini-pro\"\n",
        ")\n",
        "\n",
        "def generate_response(prompt):\n",
        "\n",
        "    chat = client.start_chat(history=[])\n",
        "\n",
        "    AI_Response = chat.send_message(\n",
        "        \"Mevsimler neden oluşur?\",\n",
        "        generation_config=genai.GenerationConfig(\n",
        "            temperature=0,\n",
        "            #max_output_tokens=256\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return AI_Response.text\n",
        "\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "st.header(\"Gemini ile İletişim Kurun\")\n",
        "st.divider()\n",
        "\n",
        "prompt = st.text_input(\"Mesajınızı Giriniz:\")\n",
        "submit_btn = st.button(\"Gönder\")\n",
        "\n",
        "if submit_btn:\n",
        "    response = generate_response(prompt)\n",
        "    st.markdown(response)"
      ],
      "metadata": {
        "id": "VZYo6xASy1XI",
        "outputId": "a5e2b09c-e709-4e61-eeb7-5199a1387b8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting gemini_v2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "id": "mAxEx54ey1Ew",
        "outputId": "f3fa2eb7-689f-4ba1-cfba-293c9a20f96b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.758s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/gemini_v2.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "Z8xCQO8karKF"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "lshNdQFXauTY",
        "outputId": "47c8cf81-1e85-4bf6-921c-e8ca11a91a16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.492s\n",
            "your url is: https://witty-dots-speak.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}