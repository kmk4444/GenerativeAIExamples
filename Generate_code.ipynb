{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTgFhOSqiOsxxZ7cuM6muP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/LLM/blob/main/Generate_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are different models to develop codes.\n",
        "- GPT-4 Turbo by OpenAI (General models)\n",
        "- Gemini Pro by Google AI (General models\n",
        "- Calude 2.1 by ANTHROP\\C (General models)\n",
        "- Github Coplit (Specialized Models)\n",
        "- Tabnine (Specialized Models)\n",
        "- Amazon CodeWhisperer (Specialized Models)\n",
        "- Replit Ghostwriter (Specialized Models)\n",
        "\n",
        "Specialized models can be utilized like extension."
      ],
      "metadata": {
        "id": "ySPU_tsDKWZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Prompt Schema**\n",
        "- You are a **PYTHON** expert.\n",
        "- I am developing a **WEB APPLICATION.**\n",
        "- You will help me with my problem.\n",
        "- I have the following **PYTHON STREAMLIT** code. Review the code and help me for **EXCEPTION HANDLING**. I want you to write the necessaary code lines or revise existing ones to let me *throw an error when the AI_Response is null, none or empty. *\n",
        "\n",
        "- Here IS MY CODE:\n",
        "-Code lines\n",
        "-Code lines\n",
        "-...\n",
        "\n",
        "- Now give me the entire code as I requested."
      ],
      "metadata": {
        "id": "lBVb1xiRM1ND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**- Requirements.txt**\n"
      ],
      "metadata": {
        "id": "HX0hTK6pO_0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!touch requirements.txt\n",
        "!echo python-dotenv >> requirements.txt\n",
        "!echo openai >> requirements.txt\n",
        "!echo streamlit >> requirements.txt\n",
        "!echo google-generativeai >> requirements.txt\n",
        "!echo anthropic >> requirements.txt\n",
        "!echo cohere >> requirements.txt\n",
        "!echo replicate >> requirements.txt"
      ],
      "metadata": {
        "id": "3r51_xH9PCjK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **terminal / bash komutu**"
      ],
      "metadata": {
        "id": "0wcry4IjPFei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfDx49IcPD-x",
        "outputId": "52c08199-06cc-4d3f-c2d8-443fadf3ec6b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: watchdog, types-requests, smmap, python-dotenv, httpx-sse, h11, fastavro, pydeck, httpcore, gitdb, httpx, gitpython, replicate, openai, cohere, anthropic, streamlit\n",
            "Successfully installed anthropic-0.25.6 cohere-5.3.3 fastavro-1.9.4 gitdb-4.0.11 gitpython-3.1.43 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 httpx-sse-0.4.0 openai-1.23.6 pydeck-0.9.0b1 python-dotenv-1.0.1 replicate-0.25.2 smmap-5.0.1 streamlit-1.33.0 types-requests-2.31.0.20240406 watchdog-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import helper\n",
        "\n",
        "# We create session_state.messages if it is not available.\n",
        "if \"messages\" not in st.session_state:\n",
        "  st.session_state.messages = []\n",
        "  #st.session_state.messages.append({\"role\": \"system\", \"content\":\"Sen yardımsever bir asistansın.\"})\n",
        "\n",
        "def adjust_model_relations():\n",
        "  st.session_state.messages= []\n",
        "\n",
        "def send_message(prompt):\n",
        "  st.chat_message(\"user\").markdown(prompt)\n",
        "\n",
        "  if st.session_state.selected_model == \"GPT-4 Turbo\": # if user select GPT-4 Turbo, it will run.\n",
        "    response = helper.gpt_generate_response(prompt)\n",
        "  elif st.session_state.selected_model == \"Gemini Pro\":\n",
        "    response = helper.gemini_generate_response(prompt)\n",
        "  elif st.session_state.selected_model == \"Claude 2.1\":\n",
        "    response = helper.command_generate_response(prompt)\n",
        "  elif st.session_state.selected_model == \"Command\":\n",
        "    response = helper.command_generate_response(prompt)\n",
        "\n",
        "  with st.chat_message(\"assistant\"):\n",
        "    st.markdown(response)\n",
        "\n",
        "st.sidebar.header(\"KONFİGÜRASYON\")\n",
        "st.sidebar.divider()\n",
        "models_list = [\"GPT-4 Turbo\", \"Gemini Pro\", \"Claude 2.1\", \"Command\"]\n",
        "st.sidebar.selectbox(label=\"Dil Modeli Seçiniz:\",options=models_list, on_change=adjust_model_relations, key=\"selected_model\")\n",
        "st.sidebar.divider()\n",
        "questions_list = [\n",
        "    \"İki sayının toplamını bulan Python kodunu yaz\",\n",
        "    \"Python Streamlit kullanarak basit kullanıcı girişi kodu yaz\",\n",
        "    \"Python Streamlit kullanarak Eisenhower Zaman Yönetimi Matrisi ekranda gösteren bir uygulamanın kodunu yaz\",\n",
        "    \"HTML, CSS ve Javascript dillerini kullanarak nihai çıktısı tek bir HTML dosyası olacak şekilde, bir sayı tahmin oyununun kodunu yaz\",\n",
        "    \"Manuel Soru Yaz\"\n",
        "    ]\n",
        "st.sidebar.selectbox(label=\"Örnek Soru Seçiniz:\", options=questions_list, key=\"selected_question\",index=4)\n",
        "\n",
        "st.title(\"Dil Modellerinin Kod Yazma Yeteneklerinin Kıyaslanması\")\n",
        "st.divider()\n",
        "\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "if st.session_state.selected_question != \"Manuel Soru Yaz\":\n",
        "  prompt = st.session_state.selected_question\n",
        "  send_message(prompt)\n",
        "\n",
        "elif prompt := st.chat_input(\"Mesajınızı Giriniz\", key=\"prompt_box\"):\n",
        "  send_message(prompt)\n",
        "  #st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-1S0lQtUKkR",
        "outputId": "ead73fd1-8c73-4097-91da-be04752c4495"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile helper.py\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import google.generativeai as genai\n",
        "import anthropic\n",
        "import cohere\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "#openai_key = os.getenv(\"openai_apikey\")\n",
        "\n",
        "client_gpt = OpenAI(api_key=\"---\")\n",
        "##############################################################\n",
        "#google_key = os.getenv(\"google_apikey\")\n",
        "\n",
        "genai.configure(api_key=\"---\")\n",
        "\n",
        "client_gemini = genai.GenerativeModel(model_name=\"gemini-pro\")\n",
        "##############################################################\n",
        "\n",
        "#anthropic_key = os.getenv(\"anthropic_apikey\")\n",
        "\n",
        "client_claude = anthropic.Anthropic(api_key=\"-----\")\n",
        "##############################################################\n",
        "\n",
        "#cohere_key = os.getenv(\"cohere_apikey\")\n",
        "\n",
        "client = cohere.Client(api_key=\"---\")\n",
        "##############################################################\n",
        "##############################################################\n",
        "\n",
        "def gpt_generate_response(prompt):\n",
        "\n",
        "    AI_Response = client_gpt.chat.completions.create(\n",
        "\n",
        "        model = \"gpt-4-1106-preview\",\n",
        "        temperature=0,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\":\"Sen yardımsever bir asistansın.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return AI_Response.choices[0].message.content\n",
        "\n",
        "\n",
        "def gemini_generate_response(prompt):\n",
        "\n",
        "    chat = client_gemini.start_chat(history=[])\n",
        "\n",
        "    AI_Response = chat.send_message(\n",
        "        prompt,\n",
        "        generation_config=genai.GenerationConfig(\n",
        "            temperature=0,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return AI_Response.text\n",
        "\n",
        "\n",
        "def claude_generate_response(prompt):\n",
        "\n",
        "    AI_Response = client_claude.beta.messages.create(\n",
        "        model = \"claude-2.1\",\n",
        "        temperature=0,\n",
        "        max_tokens=1000,\n",
        "        messages=[\n",
        "            {\"role\":\"user\", \"content\":prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return AI_Response.content[0].text\n",
        "\n",
        "\n",
        "def command_generate_response(prompt):\n",
        "\n",
        "    AI_Response = client.chat(\n",
        "        model = \"command\",\n",
        "        temperature=0,\n",
        "        message=prompt\n",
        "    )\n",
        "\n",
        "    return AI_Response.text\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MYV67kFC7nH9",
        "outputId": "478c860f-52d1-4e49-b344-ce71e63bc7a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing helper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "waUnI-596Q2J",
        "outputId": "926b43ee-acb0-437a-a163-5fc9fec65e50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.917s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.351s\n",
            "your url is: https://fine-horses-jump.loca.lt\n"
          ]
        }
      ]
    }
  ]
}